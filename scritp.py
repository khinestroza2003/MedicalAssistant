import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from typing import List, Dict, Optional

class WebConsultasScraperLimpio:
    def __init__(self):
        self.base_url = 'https://www.webconsultas.com'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Palabras a filtrar (navegaciÃ³n, menÃºs, etc)
        self.palabras_basura = [
            'portada', 'noticias', 'leer mÃ¡s', 'tambiÃ©n te puede interesar',
            'lo mÃ¡s leÃ­do', 'entrevista', 'mÃ¡s info', 'compartir contenido',
            'quiÃ©nes somos', 'ajuste de privacidad', 'belleza y bienestar',
            'mente y emociones', 'bebÃ©s y niÃ±os', 'embarazo', 'ejercicio y deporte',
            'test de psicologÃ­a', 'mÃ©todos anticonceptivos', 'actividades fitness'
        ]
    
    def delay(self, seconds: float = 1.5):
        time.sleep(seconds)
    
    def es_texto_basura(self, texto: str) -> bool:
        """Detecta si el texto es navegaciÃ³n o contenido no relevante"""
        texto_lower = texto.lower()
        
        # Demasiado corto
        if len(texto) < 15:
            return True
        
        # Contiene palabras de navegaciÃ³n
        if any(palabra in texto_lower for palabra in self.palabras_basura):
            return True
        
        # Es un enlace de navegaciÃ³n (empieza con nÃºmeros)
        if re.match(r'^\d+[A-Z]', texto):
            return True
        
        # Contiene muchas mayÃºsculas seguidas (probablemente un tÃ­tulo de secciÃ³n)
        if re.search(r'[A-Z]{10,}', texto):
            return True
        
        return False
    
    def limpiar_texto(self, texto: str) -> str:
        """Limpia y normaliza texto"""
        if not texto:
            return ""
        # Eliminar espacios mÃºltiples y saltos de lÃ­nea
        texto = re.sub(r'\s+', ' ', texto)
        # Eliminar puntos suspensivos al final
        texto = re.sub(r'\.{3,}$', '', texto)
        return texto.strip()
    
    def obtener_lista_enfermedades(self) -> List[Dict[str, str]]:
        """Obtiene lista completa de enfermedades del Ã­ndice principal"""
        print('\nğŸ“‹ Obteniendo lista completa de enfermedades...')
        
        todas_enfermedades = []
        letras = 'abcdefghijklmnopqrstuvwxyz'
        
        for letra in letras:
            try:
                url = f'{self.base_url}/salud-al-dia/{letra}'
                response = self.session.get(url, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Buscar enlaces en el contenido principal
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    texto = self.limpiar_texto(link.get_text())
                    
                    # Filtrar solo enlaces de enfermedades
                    if ('/salud-al-dia/' in href and 
                        texto and 
                        not self.es_texto_basura(texto) and
                        len(texto) > 3 and
                        href.count('/') >= 3):  # URLs de enfermedades tienen mÃ¡s niveles
                        
                        url_completa = href if href.startswith('http') else f'{self.base_url}{href}'
                        
                        # Evitar duplicados
                        if not any(e['url'] == url_completa for e in todas_enfermedades):
                            todas_enfermedades.append({
                                'nombre': texto,
                                'url': url_completa
                            })
                
                self.delay(0.5)
                print(f'   {letra.upper()}: {len([e for e in todas_enfermedades if e["nombre"][0].lower() == letra])} enfermedades')
                
            except Exception as e:
                print(f'   Error en letra {letra}: {e}')
                continue
        
        print(f'\nâœ“ Total encontradas: {len(todas_enfermedades)} enfermedades')
        return todas_enfermedades
    
    def extraer_contenido_limpio(self, soup: BeautifulSoup, tipo: str) -> List[str]:
        """Extrae contenido limpio de listas y pÃ¡rrafos"""
        contenido = []
        
        # Estrategia 1: Buscar listas (ul, ol) - mÃ¡s probable que sean sÃ­ntomas/causas
        for lista in soup.find_all(['ul', 'ol']):
            # Verificar que la lista estÃ¡ en el contenido principal, no en el menÃº
            parent = lista.find_parent(['nav', 'header', 'footer', 'aside'])
            if parent:
                continue
            
            for item in lista.find_all('li', recursive=False):
                texto = self.limpiar_texto(item.get_text())
                
                if (texto and 
                    not self.es_texto_basura(texto) and
                    15 < len(texto) < 300):
                    contenido.append(texto)
        
        # Estrategia 2: Si no hay listas suficientes, buscar en pÃ¡rrafos especÃ­ficos
        if len(contenido) < 3:
            keywords = {
                'sintomas': ['sÃ­ntoma', 'manifestaciÃ³n', 'signo', 'presenta', 
                            'dolor', 'fiebre', 'inflamaciÃ³n', 'nÃ¡usea', 'sensaciÃ³n'],
                'causas': ['causa', 'debido', 'provocado', 'origina', 'factor',
                          'desencadena', 'produce', 'consecuencia']
            }
            
            palabras_clave = keywords.get(tipo, keywords['sintomas'])
            
            for p in soup.find_all('p'):
                # Ignorar pÃ¡rrafos en navegaciÃ³n
                parent = p.find_parent(['nav', 'header', 'footer', 'aside'])
                if parent:
                    continue
                
                texto_completo = p.get_text()
                
                # Buscar pÃ¡rrafos relevantes
                if any(keyword in texto_completo.lower() for keyword in palabras_clave):
                    # Dividir en oraciones
                    oraciones = re.split(r'[.;]', texto_completo)
                    for oracion in oraciones:
                        oracion = self.limpiar_texto(oracion)
                        if (oracion and 
                            not self.es_texto_basura(oracion) and
                            20 < len(oracion) < 400):
                            contenido.append(oracion)
        
        # Eliminar duplicados manteniendo orden
        contenido_limpio = []
        for item in contenido:
            if item not in contenido_limpio:
                contenido_limpio.append(item)
        
        return contenido_limpio[:20]  # MÃ¡ximo 20 items
    
    def extraer_detalles_enfermedad(self, url: str, nombre: str) -> Optional[Dict]:
        """Extrae informaciÃ³n detallada de una enfermedad"""
        try:
            print(f'\n   ğŸ” {nombre}')
            
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Limpiar el nombre (puede venir con texto extra)
            nombre_limpio = nombre.split('\n')[0].strip()
            if len(nombre_limpio) > 35:
                nombre_limpio = nombre_limpio[:35]
            
            enfermedad = {
                'nombre': nombre_limpio,
                'url': url,
                'descripcion': '',
                'sintomas': [],
                'causas': [],
                'tratamiento': []
            }
            
            # Extraer descripciÃ³n (buscar el primer pÃ¡rrafo significativo en article)
            article = soup.find('article') or soup.find('main')
            if article:
                for p in article.find_all('p', limit=10):
                    # Ignorar pÃ¡rrafos en navegaciÃ³n
                    if p.find_parent(['nav', 'header', 'footer']):
                        continue
                    
                    texto = self.limpiar_texto(p.get_text())
                    if texto and not self.es_texto_basura(texto) and len(texto) > 80:
                        enfermedad['descripcion'] = texto[:400]
                        break
            
            # Buscar pÃ¡ginas especÃ­ficas de sÃ­ntomas y causas
            link_sintomas = soup.find('a', href=re.compile(r'sintomas|sÃ­ntomas', re.I))
            link_causas = soup.find('a', href=re.compile(r'causas', re.I))
            
            # Extraer SÃNTOMAS
            if link_sintomas:
                url_sintomas = link_sintomas.get('href')
                if not url_sintomas.startswith('http'):
                    url_sintomas = f'{self.base_url}{url_sintomas}'
                
                print(f'      â†’ SÃ­ntomas: {url_sintomas}')
                self.delay(1)
                
                try:
                    resp = self.session.get(url_sintomas, timeout=15)
                    soup_sintomas = BeautifulSoup(resp.content, 'html.parser')
                    enfermedad['sintomas'] = self.extraer_contenido_limpio(soup_sintomas, 'sintomas')
                except:
                    enfermedad['sintomas'] = self.extraer_contenido_limpio(soup, 'sintomas')
            else:
                enfermedad['sintomas'] = self.extraer_contenido_limpio(soup, 'sintomas')
            
            # Extraer CAUSAS
            if link_causas:
                url_causas = link_causas.get('href')
                if not url_causas.startswith('http'):
                    url_causas = f'{self.base_url}{url_causas}'
                
                print(f'      â†’ Causas: {url_causas}')
                self.delay(1)
                
                try:
                    resp = self.session.get(url_causas, timeout=15)
                    soup_causas = BeautifulSoup(resp.content, 'html.parser')
                    enfermedad['causas'] = self.extraer_contenido_limpio(soup_causas, 'causas')
                except:
                    enfermedad['causas'] = self.extraer_contenido_limpio(soup, 'causas')
            else:
                enfermedad['causas'] = self.extraer_contenido_limpio(soup, 'causas')
            
            # Buscar tratamiento
            link_tratamiento = soup.find('a', href=re.compile(r'tratamiento', re.I))
            if link_tratamiento:
                url_trat = link_tratamiento.get('href')
                if not url_trat.startswith('http'):
                    url_trat = f'{self.base_url}{url_trat}'
                
                self.delay(1)
                try:
                    resp = self.session.get(url_trat, timeout=15)
                    soup_trat = BeautifulSoup(resp.content, 'html.parser')
                    enfermedad['tratamiento'] = self.extraer_contenido_limpio(soup_trat, 'tratamiento')
                except:
                    pass
            
            print(f'      âœ“ {len(enfermedad["sintomas"])} sÃ­ntomas | {len(enfermedad["causas"])} causas')
            
            self.delay(1.5)
            
            # Solo retornar si tiene contenido Ãºtil
            if enfermedad['sintomas'] or enfermedad['causas'] or enfermedad['descripcion']:
                return enfermedad
            return None
            
        except Exception as e:
            print(f'      âŒ Error: {e}')
            return None
    
    def guardar_json(self, enfermedades: List[Dict], archivo: str = 'enfermedades_limpio.json'):
        """Guarda en JSON"""
        with open(archivo, 'w', encoding='utf-8') as f:
            json.dump(enfermedades, f, ensure_ascii=False, indent=2)
        print(f'\nâœ… {archivo} - {len(enfermedades)} enfermedades')
    
    def guardar_csv(self, enfermedades: List[Dict], archivo: str = 'enfermedades_limpio.csv'):
        """Guarda en CSV"""
        with open(archivo, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f, delimiter=';')
            writer.writerow(['Nombre', 'URL', 'DescripciÃ³n', 'SÃ­ntomas', 'Causas', 'Tratamiento'])
            
            for e in enfermedades:
                writer.writerow([
                    e['nombre'],
                    e['url'],
                    e['descripcion'],
                    ' | '.join(e['sintomas']),
                    ' | '.join(e['causas']),
                    ' | '.join(e.get('tratamiento', []))
                ])
        print(f'âœ… {archivo} - {len(enfermedades)} enfermedades')
    
    def ejecutar(self, max_enfermedades: int = None):
        """Ejecuta el scraping"""
        print('='*70)
        print('ğŸš€ SCRAPER WEBCONSULTAS - VERSIÃ“N LIMPIA')
        print('='*70)
        
        # Obtener lista completa de enfermedades
        lista_enfermedades = self.obtener_lista_enfermedades()
        
        if max_enfermedades:
            lista_enfermedades = lista_enfermedades[:max_enfermedades]
            print(f'\nâš™ï¸  Procesando solo {max_enfermedades} enfermedades...')
        
        todas_las_enfermedades = []
        total = len(lista_enfermedades)
        
        print(f'\n{"="*70}')
        print(f'ğŸ“Š Extrayendo detalles de {total} enfermedades...')
        print(f'{"="*70}')
        
        for idx, enfermedad in enumerate(lista_enfermedades, 1):
            print(f'\n[{idx}/{total}]', end='')
            
            detalles = self.extraer_detalles_enfermedad(
                enfermedad['url'],
                enfermedad['nombre']
            )
            
            if detalles:
                todas_las_enfermedades.append(detalles)
        
        print(f'\n{"="*70}')
        print(f'âœ… COMPLETADO: {len(todas_las_enfermedades)}/{total} enfermedades extraÃ­das')
        print(f'{"="*70}\n')
        
        if todas_las_enfermedades:
            self.guardar_json(todas_las_enfermedades)
            self.guardar_csv(todas_las_enfermedades)
            print(f'\nğŸ“‚ Archivos guardados con informaciÃ³n LIMPIA')
        
        return todas_las_enfermedades


# ==================== EJECUCIÃ“N ====================

if __name__ == '__main__':
    scraper = WebConsultasScraperLimpio()
    
    print('\nğŸ”§ CONFIGURACIÃ“N\n')
    print('Opciones:')
    print('  1. Prueba: 10 enfermedades (~10 minutos)')
    print('  2. Media: 50 enfermedades (~45 minutos)')
    print('  3. Completa: TODAS (VARIAS HORAS)\n')
    
    # CAMBIA ESTA CONFIGURACIÃ“N
    opcion = 3  # Cambiar a 2 o 3
    
    if opcion == 1:
        print('âœ“ Modo: PRUEBA (10 enfermedades)\n')
        enfermedades = scraper.ejecutar(max_enfermedades=1)
    elif opcion == 2:
        print('âœ“ Modo: MEDIA (50 enfermedades)\n')
        enfermedades = scraper.ejecutar(max_enfermedades=50)
    else:
        print('âœ“ Modo: COMPLETA (TODAS las enfermedades)\n')
        print('âš ï¸  Esto puede tardar VARIAS HORAS\n')
        enfermedades = scraper.ejecutar()
    
    print(f'\nğŸ‰ Â¡FINALIZADO! {len(enfermedades)} enfermedades procesadas')
    print(f'ğŸ“ Archivos: enfermedades_limpio.json y enfermedades_limpio.csv\n')